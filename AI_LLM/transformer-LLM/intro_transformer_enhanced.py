## ç®€å•çš„è¯­è¨€æ¨¡å‹
## å¿…é¡»å…ˆå®‰è£… transformers å’Œ torch

# åŸºæœ¬å¯¼å…¥
from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM


def create_simple_llm():
    """
    åˆ›å»ºä¸€ä¸ªä½¿ç”¨å°å‹ GPT-2 æ¨¡å‹çš„ç®€å•è¯­è¨€æ¨¡å‹ã€‚
    GPT-2ï¼ˆæœ€å°ç‰ˆæœ¬ï¼‰éå¸¸é€‚åˆæ¼”ç¤ºï¼Œå› ä¸ºå®ƒï¼š
    - ç›¸å¯¹è¾ƒå°ï¼ˆ124M å‚æ•°ï¼‰
    - åœ¨ CPU ä¸Šè¿è¡Œé€Ÿåº¦è¶³å¤Ÿå¿«
    - é€‚åˆç†è§£åŸºæœ¬æ¦‚å¿µ
    """
    # åˆå§‹åŒ–æ¨¡å‹å’Œåˆ†è¯å™¨
    model_name = "uer/gpt2-chinese-cluecorpussmall"  # ä½¿ç”¨ DistilGPT-2ï¼ˆGPT-2 çš„å°å‹ç‰ˆæœ¬ï¼‰

    # åˆ›å»ºç”Ÿæˆå™¨ç®¡é“
    generator = pipeline("text-generation", model=model_name, pad_token_id=50256)

    return generator


def generate_text(generator, prompt, max_length=1000):
    """
    åŸºäºæç¤ºç”Ÿæˆæ–‡æœ¬
    """
    # ç”Ÿæˆæ–‡æœ¬
    result = generator(
        prompt,  # è¾“å…¥æ–‡æœ¬
        max_length=max_length,  # ç”Ÿæˆæ–‡æœ¬çš„æœ€å¤§é•¿åº¦
        num_return_sequences=1,  # ç”Ÿæˆçš„ä¸åŒåºåˆ—æ•°é‡
        do_sample=True,  # ä½¿ç”¨é‡‡æ ·è€Œä¸æ˜¯è´ªå©ªè§£ç 
        temperature=0.7,  # é‡‡æ ·çš„éšæœºæ€§ï¼ˆ0 = ç¡®å®šæ€§ï¼Œ1 = éšæœºï¼‰
    )

    return result[0]["generated_text"]


# æ•™è‚²æ¼”ç¤º
def run_llm_demo():
    """
    æ¼”ç¤ºåŸºæœ¬çš„è¯­è¨€æ¨¡å‹åŠŸèƒ½å¹¶é™„å¸¦è§£é‡Š
    """
    print("ğŸ¤– æ­£åœ¨åŠ è½½ç®€å•è¯­è¨€æ¨¡å‹...")
    generator = create_simple_llm()

    print("\nâœ¨ ç®€å•è¯­è¨€æ¨¡å‹æ¼”ç¤º âœ¨")
    print("è¿™ä¸ªæ¼”ç¤ºå±•ç¤ºäº†ä½¿ç”¨å°å‹è¯­è¨€æ¨¡å‹çš„åŸºæœ¬æ–‡æœ¬ç”Ÿæˆ")

    # ç”¨äºæ¼”ç¤ºä¸åŒåŠŸèƒ½çš„ç¤ºä¾‹æç¤º
    prompts = [
        "å¿«é€Ÿçš„æ£•è‰²ç‹ç‹¸",
        "å¾ˆä¹…å¾ˆä¹…ä»¥å‰",
        "Pythonç¼–ç¨‹æ˜¯",
    ]

    for prompt in prompts:
        print("\nğŸ”¹ æç¤º:", prompt)
        print("ğŸ”¸ ç”Ÿæˆ:", generate_text(generator, prompt))
        input("\næŒ‰å›è½¦é”®æŸ¥çœ‹ä¸‹ä¸€ä¸ªç¤ºä¾‹...")


# äº¤äº’å¼æ¼”ç¤º
def interactive_demo():
    """
    å…è®¸ç”¨æˆ·ä¸æ¨¡å‹äº¤äº’
    """
    generator = create_simple_llm()

    print("\nğŸ¤– äº¤äº’å¼è¯­è¨€æ¨¡å‹æ¼”ç¤º")
    print("è¾“å…¥ä½ çš„æç¤ºï¼ˆè¾“å…¥ 'quit' é€€å‡ºï¼‰")

    while True:
        prompt = input("\nâœï¸ è¾“å…¥ä½ çš„æç¤º: ")
        if prompt.lower() == "quit":
            break

        response = generate_text(generator, prompt)
        print("\nğŸ’­ ç”Ÿæˆçš„å“åº”:")
        print(response)


# è¿‡ç¨‹çš„æ•™è‚²å¯è§†åŒ–
def explain_process():
    """
    é€šè¿‡ç®€å•ç¤ºä¾‹è§£é‡Šè¯­è¨€æ¨¡å‹çš„å·¥ä½œè¿‡ç¨‹
    """
    print("\nğŸ“ å·¥ä½œåŸç†:")
    print("1. è¾“å…¥æ–‡æœ¬ â†’ åˆ†è¯ â†’ æ•°å­—")
    print("2. æ•°å­— â†’ æ¨¡å‹å¤„ç† â†’ é¢„æµ‹")
    print("3. é¢„æµ‹ â†’ æ–°æ ‡è®° â†’ è¾“å‡ºæ–‡æœ¬")

    # ç®€å•çš„åˆ†è¯ç¤ºä¾‹
    tokenizer = AutoTokenizer.from_pretrained("distilgpt2")
    text = "Hello world!"
    tokens = tokenizer.encode(text)
    decoded = tokenizer.decode(tokens)

    print("\nğŸ“ åˆ†è¯ç¤ºä¾‹:")
    print(f"åŸå§‹æ–‡æœ¬: '{text}'")
    print(f"è½¬æ¢ä¸ºæ ‡è®°ï¼ˆæ•°å­—ï¼‰: {tokens}")
    print(f"è§£ç å›æ–‡æœ¬: '{decoded}'")


if __name__ == "__main__":
    print("é€‰æ‹©æ¼”ç¤º:")
    print("1. è¿è¡ŒåŸºæœ¬æ¼”ç¤º")
    print("2. äº¤äº’æ¨¡å¼")
    print("3. è§£é‡Šå·¥ä½œè¿‡ç¨‹")

    choice = input("è¾“å…¥ä½ çš„é€‰æ‹© (1-3): ")

    if choice == "1":
        run_llm_demo()
    elif choice == "2":
        interactive_demo()
    elif choice == "3":
        explain_process()
    else:
        print("æ— æ•ˆçš„é€‰æ‹©!")